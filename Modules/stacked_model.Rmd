```{r stacked_load, include=FALSE}
log_prob <- readRDS("../Data/improved_log_prob.rds")
knn_pred <- readRDS("../Data/knn_pred.rds")
knn_prob <- readRDS("../Data/knn_prob.rds")
ann_pred <- readRDS("../Data/ann_prob.rds")
rf_pred <- readRDS("../Data/rf_tuned_prediction.rds")
svm_pred <- readRDS("../Data/svm_prediction.rds")
decision_tree_pred <- readRDS("../Data/decision_tree_pred.rds")
load("../Data/SpotifyTrainTest.Rdata")
library(C50)
library(caret)
```

## Combine Data

We are going to take all the data from the different individual models and combine them to make a new training set.
```{r combined_data}
spotify_preds <- data.frame(
  log = log_prob,
  knn_pred = knn_pred,
  knn_prob = knn_prob,
  ann = ann_pred,
  svm = svm_pred,
  decision_tree = decision_tree_pred,
  rf = rf_pred,
  true = as.factor(spotify_test$track_popularity)
)
```


## Train and Test Sets

We are going to make a train and test split for the new combined data set, with 70\% of the data going to the train set.
```{r combined_split}
set.seed(12345)
combined_split <- 0.7

tree_train_rows <- sample(1:nrow(spotify_preds), combined_split*nrow(spotify_preds))
tree_train <- spotify_preds[tree_train_rows,]
tree_test <- spotify_preds[-tree_train_rows,]
```

## Combined Tree
Next we make the combined model decision tree with a specified cost matrix with values c(0, 2, 2, 0). This will allow us to focus mainly on the false positives and false negatives of our final predictions, assigning equal cost weights to each of these. This will further help us determine the penalty assigned to having more false negatives than false positives and vice versa.

```{r combined_tree, warning=FALSE}
set.seed(12345)
cost <- matrix(c(0, 2, 2, 0), nrow = 2)
tree_model <- C5.0(true ~ ., data = tree_train, cost = cost)
```

Below, we construct a confusion matrix to obtain the accuracy score, kappa value, and distribution of false negatives and false positives for this stacked model.
```{r combined_confusion}
tree_predict <- predict(tree_model, tree_test)

print(confusionMatrix(as.factor(tree_predict), as.factor(tree_test$true), positive = "1"))
summary(tree_model)
```

The following diagram shows the structure of the decision tree with the splitting nodes indicated at each level. For example, the logistic regression results collectively served as the root splitting node for the tree. This is because, out of all models, splitting on logistic regression yielded the greatest entropy gain by most effectively separating the true y values (i.e. track_popularity) into separate distinguishable buckets depending on the output of the logistic regression model.
```{r}
plot(tree_model)
```

The accuracy is 0.6197 and the final kappa is 0.2397. Using this information, we see that the final comprehensive model was able to perform slightly better than other individual models, like the decision tree. Overall, we notice that the stacked model was able to perform more accurately and yield better predictive performance, as it combined predictions from all models into a single dataframe for training purposes.

The accuracy and kappa above are the final metrics for the prediction of track popularity given Spotify songs with attributes such as danceability, energy, accousticness, and more.

## Conclusion
Given the above accuracy and kappa, how can this model be scaled to benefit the music industry?

In our analysis, we assume our audience to be both individual song artists and record labels. This model can be useful for song artists, as is will allow them to determine the predicted popularity of their new song given significant metrics such as loudness, accousticness, tempo, and more. Moreover, if an artist solely has an idea for a new song but has not completed it yet, this model can help to gauge the popularity of the *type* of song the artist is planning to craft.

Let's take, for example, a song called "Dealer" by Troy Murray. It has the following attributes:
Danceability: 0.713
Energy: 0.649
Key: 9
Loudness: -10.353
Mode: 0
Speechiness: 0.0586
Accousticness: 0.113000
Instrumentalness: 0.0453
Liveness: 0.175
Valence: 0.3200
Tempo: 101.034

Given this song's attributes, our model predicted it to have a track popularity score of 0. Comparing this score with the true track popularity assigned to this song, we see that the classification was correct. Serving as a supplemental guide, this model could help artists like Troy understand the projected popularity of their songs in order to gauge how much money (if any) to spend on marketing their new song depending on whether the general population is predicted to enjoy it. This also helps artists like Troy determine whether they should spend time making similar songs in the future or potentially reinvent their music-making strategy to make songs that better appeal to a larger audience of listeners.

In terms of record labels, it makes sense to sign artists that have a high proportion of songs with high track_popularity such that their songs are predicted to be popular. This way, record labels know prior to signing a candidate that they are on track to reach a broader audience and potentially profit more from popular songs. This also allows record labels, or the music industry, to decide which songs it should spend more money advertising. By selecting songs classified as being popular by our model, these agencies reduce the risk of over-spending on unpopular songs that don't end up collecting enough money to break even or profit. 

### Limitations
Though our model helps relate multiple distinct song attributes to a final popularity outcome, it is still possible for the model to misclassify songs, surfacing a trade-off between false negatives (i.e. songs predicted to be unpopular but are actually popular) and false positives (i.e. songs predicted to be popular but are actually not). Due to the nature of the problem, it would be unreasonable to completely eliminate either of these, as one could make the argument that having fewer songs predicted to be popular can make the model's decision more strict and almost guarantee that any given song whose predicted popularity is 1 will be popular; however, this degree of constraint discourages the generation and promotion of songs that would have truly been popular but missed the threshold of classification set by the model.

Additionally, since the music industry, listeners' preferences, and types of popular songs are always changing, it may not be the case that our model could perform well in the future. In order to ensure that our model is up-to-date with new songs, we'd need to continuously train it using songs that get released. Though, given the breadth of songs available in the industry today, it may not be feasible to train our model in a timely manner such that it understands the popularity trends for every single song.

Nevertheless, we recommend our model's final result to be used as supplemental information to guide an artist or record label's understanding of the potential popularity of a song. Though our model may not be a complete source of truth given it has not obtained perfect accuracy, it can still serve as a critical resource to understand the general acceptance of various types of songs in the music industry given attributes such as loudness, accousticness, tempo, and many more.







```{r stacked_load, include=FALSE}
log_prob <- readRDS("../Data/improved_log_prob.rds")
knn_pred <- readRDS("../Data/knn_pred.rds")
knn_prob <- readRDS("../Data/knn_prob.rds")
ann_pred <- readRDS("../Data/ann_prob.rds")
rf_pred <- readRDS("../Data/rf_tuned_prediction.rds")
svm_pred <- readRDS("../Data/svm_prediction.rds")
decision_tree_pred <- readRDS("../Data/decision_tree_pred.rds")
load("../Data/SpotifyTrainTest.Rdata")
library(C50)
```

## Combine Data

We are going to take all the data from the different individual models and combine them to make a new training set.
```{r combined_data}
spotify_preds <- data.frame(
  log = log_prob,
  knn_pred = knn_pred,
  knn_prob = knn_prob,
  ann = ann_pred,
  svm = svm_pred,
  decision_tree = decision_tree_pred,
  rf = rf_pred,
  true = as.factor(spotify_test$track_popularity)
)
```


## Train and Test Sets

We are going to make a train and test split for the new combined data set, with 70\% of the data going to the train set.
```{r combined_split}
set.seed(12345)
combined_split <- 0.7

tree_train_rows <- sample(1:nrow(spotify_preds), combined_split*nrow(spotify_preds))
tree_train <- spotify_preds[tree_train_rows,]
tree_test <- spotify_preds[-tree_train_rows,]
```

## Combined Tree
Next we make the combined model decision tree with a specified cost matrix with values c(0, 2, 2, 0). This will allow us to focus mainly on the false positives and false negatives of our final predictions, assigning equal cost weights to each of these. This will further help us determine the penalty assigned to having more false negatives than false positives and vice versa.

```{r combined_tree, warning=FALSE}
set.seed(12345)
cost <- matrix(c(0, 2, 2, 0), nrow = 2)
tree_model <- C5.0(true ~ ., data = tree_train, cost = cost)
```

Below, we construct a confusion matrix to obtain the accuracy score, kappa value, and distribution of false negatives and false positives for this stacked model.
```{r combined_confusion}
tree_predict <- predict(tree_model, tree_test)

summary(tree_model)
print(confusionMatrix(as.factor(tree_predict), as.factor(tree_test$true), positive = "1"))
```

The accuracy is 0.6197 and the final kappa is 0.2397. Using this information, we see that the final comprehensive model was able to perform slightly better than other individual models, like the decision tree. Overall, we notice that the stacked model was able to perform more accurately and yield better predictive performance, as it combined predictions from all models into a single dataframe for training purposes.

The accuracy and kappa above are the final metrics for the prediction of track popularity given Spotify songs with attributes such as danceability, energy, accousticness, and more.

